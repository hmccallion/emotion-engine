{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02581674-137d-4ac2-9d30-c93373abf8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the dependencies\n",
    "import pickle\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "536d1c7f-0e4a-4de3-991b-a686b42bd7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b0396e-22d4-4671-9d66-b453db3a5430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mccal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50604bb0-2560-4fbe-a621-72dd87c3c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is going to be a set containing all of the english stop words\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa93918-0eb7-4c71-a565-69012fce1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e3bb3c5-67cc-4ff0-b43d-c9e1f14cfa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this pickle holds the serialized emotion detection model that i trained\n",
    "with open('emotion_model.pkl','rb') as file:\n",
    "    emotion_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c32a6b45-ff12-4f39-a15d-fd401441e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a tfdidf vectorizer that was fit with the training data\n",
    "with open('vectorizer.pkl','rb') as file:\n",
    "    vectorizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98bf572-ec9f-482f-80f1-c93ed3eebe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a data frame that has a vector column that the search algorithm depends on\n",
    "with open('scored_music.pkl','rb') as file:\n",
    "    scored_music = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b07bee9a-f1dc-445f-b77f-f0426603941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next i am going to import the functions that allow for searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7db30b3d-6211-4a2c-a99b-1263fd396dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_score(user_input):\n",
    "    # tokenize the input\n",
    "    input_tokens = nltk.word_tokenize(user_input.lower()) \n",
    "    # filter stopwords out of the input\n",
    "    filtered_input_tokens = [word for word in input_tokens if word not in stop_words] \n",
    "    # re-join the filtered tokens\n",
    "    input_combined = ' '.join(filtered_input_tokens) \n",
    "    # feature extraction\n",
    "    input_vector = vectorizer.transform([input_combined]) \n",
    "    # pull the probabilities from the input\n",
    "    probabilities = model.predict_proba(input_vector)\n",
    "    # return the probabilities\n",
    "    return probabilities.tolist()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c772a18d-d88f-493a-b136-c5a3c4ee80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that can compute the similarity between two vectors\n",
    "def song_similarity(u_vec,s_vec):\n",
    "    # convert the song vector into a numpy array\n",
    "    song_array = np.array(s_vec) \n",
    "    # convert the user vector into a numpy array\n",
    "    u_array = np.array(u_vec) \n",
    "    # reshape the song array so it fits into the cosine similarity function\n",
    "    shaped_s_array = np.array(song_array).reshape(1,-1) \n",
    "    # reshape the user array so it fits into the cosine similarity function\n",
    "    shaped_u_array = np.array(u_array).reshape(1,-1) \n",
    "    # compute the similarity\n",
    "    similar = cosine_similarity(shaped_s_array,shaped_u_array) \n",
    "    # the similarity is inside a list inside a list so we need to index it twice\n",
    "    return similar[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "028e87a6-021f-4737-902b-762a50cc2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a function that does the similarity search\n",
    "\n",
    "def cosine_similarity_search(user_input):\n",
    "    # the function takes in user input then turns it into a vector\n",
    "    user_vector = emotion_score(user_input) \n",
    "    # it initializes a list called comparison\n",
    "    comparison = [] \n",
    "    # it opens a loop that iterates through the length of the scored music data\n",
    "    for i in range(len(scored_music)):  \n",
    "        # the loop initializes a list that hold the similarity to the user vector song and artist\n",
    "        user_vs_song=[] \n",
    "        \n",
    "        # then the cosine similarity between the user vector and song vector is computed and stored as a variable \n",
    "        cos_sim = song_similarity(user_vector,scored_music['vector'][i]) \n",
    "        # the song name is stored to a variable\n",
    "        song_name = scored_music['song'][i] \n",
    "        # the artist name is stored to a variable\n",
    "        artist_name = scored_music['artist'][i] \n",
    "        \n",
    "        # these three variables are appended to the user_vs_song list\n",
    "        user_vs_song.append(cos_sim)\n",
    "        user_vs_song.append(song_name)\n",
    "        user_vs_song.append(artist_name)\n",
    "        \n",
    "        # the user_vs_song list is appended to the comparison list to later be sorted\n",
    "        comparison.append(user_vs_song)\n",
    "\n",
    "    # once the loop is done executing we convert our comparison list into a data frame so it can be sorted easiliy\n",
    "    comp_df=pd.DataFrame(comparison,columns=['similarity','track','artist'])\n",
    "    \n",
    "    # return the sorted df\n",
    "    return comp_df.sort_values(by='similarity',ascending=False)#.head(50).sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed13309d-c214-4bfc-9b14-669b29745c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can play with the search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1c03c0e-52a3-43e5-b722-03b81bd0d31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>track</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18945</th>\n",
       "      <td>0.999684</td>\n",
       "      <td>Mr Blue</td>\n",
       "      <td>Yazoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.999659</td>\n",
       "      <td>Here's Your Letter (Toronto Concert)</td>\n",
       "      <td>Avril Lavigne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40579</th>\n",
       "      <td>0.999576</td>\n",
       "      <td>I Hate You</td>\n",
       "      <td>Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26563</th>\n",
       "      <td>0.999570</td>\n",
       "      <td>Now That It's Over</td>\n",
       "      <td>Everclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8770</th>\n",
       "      <td>0.999426</td>\n",
       "      <td>Without A Shot</td>\n",
       "      <td>John Mellencamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12091</th>\n",
       "      <td>0.064902</td>\n",
       "      <td>Impressed</td>\n",
       "      <td>Natalie Imbruglia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38927</th>\n",
       "      <td>0.064463</td>\n",
       "      <td>Funny How Love Is</td>\n",
       "      <td>Queen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21066</th>\n",
       "      <td>0.064453</td>\n",
       "      <td>Funny Girl</td>\n",
       "      <td>Barbra Streisand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17252</th>\n",
       "      <td>0.064452</td>\n",
       "      <td>Funny Thing</td>\n",
       "      <td>Travis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22445</th>\n",
       "      <td>0.064416</td>\n",
       "      <td>I'm So Curious</td>\n",
       "      <td>Britney Spears</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44795 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       similarity                                 track             artist\n",
       "18945    0.999684                               Mr Blue              Yazoo\n",
       "899      0.999659  Here's Your Letter (Toronto Concert)      Avril Lavigne\n",
       "40579    0.999576                            I Hate You             Slayer\n",
       "26563    0.999570                    Now That It's Over          Everclear\n",
       "8770     0.999426                        Without A Shot    John Mellencamp\n",
       "...           ...                                   ...                ...\n",
       "12091    0.064902                             Impressed  Natalie Imbruglia\n",
       "38927    0.064463                     Funny How Love Is              Queen\n",
       "21066    0.064453                            Funny Girl   Barbra Streisand\n",
       "17252    0.064452                           Funny Thing             Travis\n",
       "22445    0.064416                        I'm So Curious     Britney Spears\n",
       "\n",
       "[44795 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_search(\"i placed at least five different bets on thursday night football and not a single one hit.  I probably lost at least forty dollars.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c6299-7a9e-481b-995d-b0cd9bfe4a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
